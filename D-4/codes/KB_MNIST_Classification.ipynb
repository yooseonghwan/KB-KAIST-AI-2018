{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Hand-written digit classification\n",
    "\n",
    "### **2018/10/13 KB-KAIST 인공지능 집중 강의**<br/>\n",
    "Authorized by SIIT, KAIST\n",
    "Yekang Lee, Jaemyung Yu, and Junmo Kim<br/>\n",
    "Converted to Jupyter notebook by Kiwon Lee\n",
    "\n",
    "***Tip> shotcuts for Jupyter Notebook***\n",
    "* Ctrl + Enter : run cell\n",
    "* Shift + Enter : run cell and select below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Libraries\n",
    "* Numpy: Fundamenta package for scientific computing with Python\n",
    "* Tensorflow: An open source machine learning library for research and production\n",
    "* Matplotlib: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.training import moving_averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "log_dir = 'logs/'\n",
    "batch_size = 100\n",
    "base_lr = 0.1\n",
    "\n",
    "HParams = namedtuple('HParams',\n",
    "                     'batch_size, lrn_rate, '\n",
    "                     'weight_decay_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a graph\n",
    "- Option 1: Resnet\n",
    "- Option 2: VGGnet\n",
    "- Option 3: Simple network (fill in the blank)\n",
    "- Option 4: DIY network (fill in the blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepnet(object):\n",
    "\n",
    "  def __init__(self, hps, mode):\n",
    "\n",
    "    self.hps = hps\n",
    "\n",
    "    ## HYPERPARAMETER\n",
    "    self.total_classes = 10\n",
    "    self.relu_leakiness = 0\n",
    "    self.mode = mode\n",
    "    self._extra_train_ops = []\n",
    "    self.num_residual_units = 3\n",
    "\n",
    "    ## PLACEHOLDERS\n",
    "    self.lrn_rate = tf.placeholder(tf.float32, shape=(), name='lrn_rate')\n",
    "    self.images = tf.placeholder(tf.float32, shape=(hps.batch_size, 28, 28, 1), name='images')\n",
    "    self.labels = tf.placeholder(tf.float32, shape=(hps.batch_size, self.total_classes), name='labels')\n",
    "\n",
    "\n",
    "  ## BUILD GRAPH\n",
    "  ## Option 1: resnet\n",
    "  ## Option 2: vggnet\n",
    "  ## Option 3: simple network (fill in the blank!)\n",
    "  ## Option 4: DIY network (fill in the blank!)\n",
    "\n",
    "  def build_graph(self):\n",
    "    ## GLOBAL ITERATION\n",
    "    self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "    ## FEATURE EXTRACTION\n",
    "    with tf.variable_scope('embed') as scope:\n",
    "      #feats = self.resnet(self.images)\n",
    "     # feats = self.vggnet(self.images)\n",
    "      feats = self.simple_network(self.images)\n",
    "      #feats = self.DIY_network(self.images)\n",
    "\n",
    "    ## LOGITS\n",
    "    logits = self._fully_connected('logits', feats, self.total_classes)\n",
    "\n",
    "    ## SOFTMAX \n",
    "    self.prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    ## COST FUNCTION (CROSS ENTROPY)\n",
    "    cent = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.labels)\n",
    "    self.cost_cls = tf.reduce_mean(cent, name='cent')\n",
    "\n",
    "    self.build_cost()\n",
    "\n",
    "    if self.mode == 'train':\n",
    "      self.build_train_op()\n",
    "\n",
    "\n",
    "  def build_cost(self):\n",
    "    if self.mode == 'train':\n",
    "      self.cost = self.cost_cls\n",
    "      self.cost += self.decay()\n",
    "    elif self.mode == 'test':\n",
    "      self.cost = self.cost_cls\n",
    "\n",
    "  ## OPTIMIZER (SGD+MOMENTUM)\n",
    "  def build_train_op(self):\n",
    "    optimizer = tf.train.MomentumOptimizer(self.lrn_rate, 0.9, use_nesterov=True)\n",
    "    train_vars = tf.trainable_variables()\n",
    "    grads = tf.gradients(self.cost, train_vars)\n",
    "    apply_op = optimizer.apply_gradients(zip(grads, train_vars),\n",
    "                                         global_step=self.global_step,\n",
    "                                         name='train_step')\n",
    "\n",
    "    train_ops = [apply_op] + self._extra_train_ops\n",
    "    self.train_op = tf.group(*train_ops)\n",
    "\n",
    "  ## L2 WEIGHT DECAY\n",
    "  def decay(self):\n",
    "    costs = []\n",
    "    for var in tf.trainable_variables():\n",
    "      if var.op.name.find(r'DW') > 0:\n",
    "        costs.append(tf.nn.l2_loss(var))\n",
    "\n",
    "    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))\n",
    "\n",
    "  ################################\n",
    "  ####### FILL IN THE BLANK ######\n",
    "  ################################\n",
    "  ################################\n",
    "  ####### BE CREATIVE! ###########\n",
    "  ################################\n",
    "  def DIY_network(self, images):\n",
    "    \"\"\" Your Code \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    img_feat = self._global_avg_pool(x)\n",
    "    return img_feat\n",
    "\n",
    "\n",
    "\n",
    "  ################################\n",
    "  ####### FILL IN THE BLANK ######\n",
    "  ################################\n",
    "  def simple_network(self, images):\n",
    "    \"\"\" Your Code \"\"\"\n",
    "\n",
    "    #conv st1\n",
    "    x1 = self._conv('st1', images, 3, 1, 16, self._stride_arr(1))\n",
    "    x1 = self._batch_norm('bn1', x1)\n",
    "    x1 = self._relu(x1, self.relu_leakiness)\n",
    "    \n",
    "     #conf st2\n",
    "    x2 = self._conv('st3', x1, 3, 16, 16, self._stride_arr(1))\n",
    "    x2 = self._batch_norm('bn1', x2)\n",
    "    x2 = self._relu(x2, self.relu_leakiness)\n",
    "\n",
    "    #conv st1\n",
    "    x1 = self._conv('st2', x1, 3, 16, 16, self._stride_arr(1))\n",
    "    x1 = self._batch_norm('bn1', x1)\n",
    "    x1 = self._relu(x1, self.relu_leakiness)    \n",
    "    \n",
    "    #avg pool st2\n",
    "    x1 = tf.nn.avg_pool(x1, self._stride_arr(2), self._stride_arr(2), 'VALID')    \n",
    "       \n",
    "         \n",
    "    #Eltwise sum\n",
    "    x= x1 + x2\n",
    "    \n",
    "    #conv st1\n",
    "    x3 = self._conv('st4', x3, 3, 16, 16, self._stride_arr(1))\n",
    "    x3 = self._batch_norm('bn1', x3)\n",
    "    x3 = self._relu(x3, self.relu_leakiness)\n",
    "    \n",
    "    \n",
    "    #concatenate sum\n",
    "    x = tf.concat([x3, x], 3)    \n",
    "    \n",
    "    \n",
    "    #conv st1\n",
    "    x = self._conv('st5', X, 3, 32, 32, self._stride_arr(1))    \n",
    "            \n",
    "    x = self._batch_norm('bn1', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    \n",
    "    \n",
    "    \n",
    "    img_feat = self._global_avg_pool(x)\n",
    "    return img_feat\n",
    "\n",
    "  ## VGGNET\n",
    "  ## Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
    "  ## https://arxiv.org/abs/1409.1556\n",
    "\n",
    "  def vggnet(self, images, labels=None):\n",
    "    x = self._conv('conv1', images, 7, 1, 16, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn1', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    x = self._conv('conv2', x, 3, 16, 16, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn2', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    x = tf.nn.avg_pool(x, self._stride_arr(2), self._stride_arr(2), 'VALID')\n",
    "\n",
    "    x = self._conv('conv3', x, 3, 16, 32, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn3', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    x = self._conv('conv4', x, 3, 32, 32, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn4', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    x = tf.nn.avg_pool(x, self._stride_arr(2), self._stride_arr(2), 'VALID')\n",
    "\n",
    "    x = self._conv('conv5', x, 3, 32, 64, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn5', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    x = self._conv('conv6', x, 3, 64, 64, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn6', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    img_feat = self._global_avg_pool(x)\n",
    "    return img_feat\n",
    "\n",
    "\n",
    "  ## RESNET\n",
    "  ## Deep Residual Learning for Image Recognition\n",
    "  ## https://arxiv.org/abs/1512.03385\n",
    "    \n",
    "  def resnet(self, images, labels=None):\n",
    "\n",
    "    with tf.variable_scope('init'):\n",
    "      x = self._conv('conv1', images, 3, 1, 16, self._stride_arr(1))\n",
    "\n",
    "    strides = [1, 2, 2]\n",
    "    activate_before_residual = [True, False, False]\n",
    "    res_func = self.residual\n",
    "    filters = [16, 16, 32, 64]\n",
    "\n",
    "    with tf.variable_scope('unit_1_0'):\n",
    "      x = res_func(x, filters[0], filters[1], self._stride_arr(strides[0]),\n",
    "                   activate_before_residual[0])\n",
    "    for i in range(1, self.num_residual_units):\n",
    "      with tf.variable_scope('unit_1_%d' % i):\n",
    "        x = res_func(x, filters[1], filters[1], self._stride_arr(1), False)\n",
    "\n",
    "    with tf.variable_scope('unit_2_0'):\n",
    "      x = res_func(x, filters[1], filters[2], self._stride_arr(strides[1]),\n",
    "                   activate_before_residual[1])\n",
    "    for i in range(1, self.num_residual_units):\n",
    "      with tf.variable_scope('unit_2_%d' % i):\n",
    "        x = res_func(x, filters[2], filters[2], self._stride_arr(1), False)\n",
    "\n",
    "    with tf.variable_scope('unit_3_0'):\n",
    "      x = res_func(x, filters[2], filters[3], self._stride_arr(strides[2]),\n",
    "                   activate_before_residual[2])\n",
    "    for i in range(1, self.num_residual_units):\n",
    "      with tf.variable_scope('unit_3_%d' % i):\n",
    "        x = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\n",
    "    \n",
    "    with tf.variable_scope('unit_last'):\n",
    "      x = self._batch_norm('bn_last', x)\n",
    "      x = self._relu(x, self.relu_leakiness)\n",
    "      img_feat = self._global_avg_pool(x)\n",
    "      return img_feat\n",
    "\n",
    "\n",
    "  def residual(self, x, in_filter, out_filter, stride,\n",
    "                activate_before_residual=False):\n",
    "    \"\"\"Residual unit with 2 sub layers.\"\"\"\n",
    "    if activate_before_residual:\n",
    "      with tf.variable_scope('shared_activation'):\n",
    "        x = self._batch_norm('bn_init', x)\n",
    "        x = self._relu(x, self.relu_leakiness)\n",
    "        orig_x = x\n",
    "    else:\n",
    "      with tf.variable_scope('residual_only_activation'):\n",
    "        orig_x = x\n",
    "        x = self._batch_norm('bn_init', x)\n",
    "        x = self._relu(x, self.relu_leakiness)\n",
    "\n",
    "    with tf.variable_scope('sub1'):\n",
    "      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)\n",
    "\n",
    "    with tf.variable_scope('sub2'):\n",
    "      x = self._batch_norm('bn2', x)\n",
    "      x = self._relu(x, self.relu_leakiness)\n",
    "      x = self._conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n",
    "\n",
    "    with tf.variable_scope('sub_add'):\n",
    "      if in_filter != out_filter:\n",
    "        orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')\n",
    "        orig_x = tf.pad(\n",
    "            orig_x, [[0, 0], [0, 0], [0, 0],\n",
    "                     [(out_filter-in_filter)//2, (out_filter-in_filter)//2]])\n",
    "      x += orig_x\n",
    "\n",
    "    return x\n",
    "\n",
    "  ## STRIDE\n",
    "  def _stride_arr(self, stride):\n",
    "    return [1, stride, stride, 1]\n",
    "  \n",
    "    3,3,13\n",
    "  ## CONVOLUTIONAL LAYER\n",
    "  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n",
    "    with tf.variable_scope(name):\n",
    "      n = filter_size * filter_size * out_filters\n",
    "      w = tf.get_variable(\n",
    "          'weight/DW', [filter_size, filter_size, in_filters, out_filters],\n",
    "          tf.float32, initializer=tf.uniform_unit_scaling_initializer(factor=2.0))\n",
    "\n",
    "      y = tf.nn.conv2d(x, w, strides, padding='SAME')\n",
    "\n",
    "    return y\n",
    "\n",
    "  ## RELU LAYER\n",
    "  def _relu(self, x, leakiness=0.0):\n",
    "    return tf.maximum(x, leakiness*x)\n",
    "\n",
    "  ## FULLY-CONNECTED LAYER\n",
    "  def _fully_connected(self, name, x, out_dim, is_reuse=None):\n",
    "    with tf.variable_scope(name, reuse=is_reuse):\n",
    "      x = tf.reshape(x, [self.hps.batch_size, -1])\n",
    "      w = tf.get_variable(\n",
    "          'weight/DW', [x.get_shape()[1], out_dim],\n",
    "          initializer=tf.uniform_unit_scaling_initializer(factor=2.0))\n",
    "      b = tf.get_variable('bias/DW', [out_dim],\n",
    "                          initializer=tf.constant_initializer())\n",
    "      y = tf.nn.xw_plus_b(x, w, b)\n",
    "\n",
    "    return y\n",
    "\n",
    "  ## GLOBAL AVERAGE POOLING LAYER\n",
    "  def _global_avg_pool(self, x):\n",
    "    assert x.get_shape().ndims == 4\n",
    "    return tf.reduce_mean(x, [1, 2])\n",
    "\n",
    "  ## BATCH NORMALIZATION LAYER\n",
    "  def _batch_norm(self, name, x):\n",
    "    with tf.variable_scope(name):\n",
    "      params_shape = [x.get_shape()[-1]]\n",
    "\n",
    "      beta = tf.get_variable(\n",
    "          'beta', params_shape, tf.float32,\n",
    "          initializer=tf.constant_initializer(0.0, tf.float32))\n",
    "      gamma = tf.get_variable(\n",
    "          'gamma', params_shape, tf.float32,\n",
    "          initializer=tf.constant_initializer(1.0, tf.float32))\n",
    "\n",
    "      if self.mode == 'train':\n",
    "        mean, variance = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
    "\n",
    "        moving_mean = tf.get_variable(\n",
    "            'moving_mean', params_shape, tf.float32,\n",
    "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
    "            trainable=False)\n",
    "        moving_variance = tf.get_variable(\n",
    "            'moving_variance', params_shape, tf.float32,\n",
    "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
    "            trainable=False)\n",
    "\n",
    "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
    "            moving_mean, mean, 0.9))\n",
    "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
    "            moving_variance, variance, 0.9))\n",
    "      else:\n",
    "        mean = tf.get_variable(\n",
    "            'moving_mean', params_shape, tf.float32,\n",
    "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
    "            trainable=False)\n",
    "        variance = tf.get_variable(\n",
    "            'moving_variance', params_shape, tf.float32,\n",
    "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
    "            trainable=False)\n",
    "\n",
    "      # epsilon used to be 1e-5. Maybe 0.001 solves NaN problem in deeper net.\n",
    "      y = tf.nn.batch_normalization(\n",
    "          x, mean, variance, beta, gamma, 0.001)\n",
    "      y.set_shape(x.get_shape())\n",
    "      return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "hps = HParams(batch_size=batch_size, lrn_rate=base_lr, weight_decay_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 3.1 Load MNIST data and split train/val/test pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "One-hot labels for this image:\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## READ MNIST INPUTS\n",
    "mnist = input_data.read_data_sets('./data/', one_hot=True)\n",
    "\n",
    "## NUM_DATA\n",
    "NUM_DATA = 1000\n",
    "\n",
    "## TRAIN/VAL SPLITS\n",
    "train_images = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "train_images = train_images.reshape([-1, 28, 28, 1])\n",
    "train_images = train_images[0:NUM_DATA]\n",
    "train_labels = train_labels[0:NUM_DATA]\n",
    "\n",
    "val_images = mnist.validation.images\n",
    "val_labels = mnist.validation.labels\n",
    "val_images = val_images.reshape([-1, 28, 28, 1])\n",
    "\n",
    "## RANDOM SHUFFLING\n",
    "order = np.random.permutation(train_images.shape[0])\n",
    "order = order.astype(np.int32)\n",
    "train_images = train_images[order, :, :, :]\n",
    "train_labels = train_labels[order, :]\n",
    "\n",
    "## Plot the 1st image and label\n",
    "plt.imshow(train_images[0,:,:,0], cmap='Greys')\n",
    "print(\"\\nOne-hot labels for this image:\")\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable embed/bn1/beta already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\KAIST\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\KAIST\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\KAIST\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2054e418ecc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## BUILD GRAPH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m## MAKE SESSION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c9cfe0b8c492>\u001b[0m in \u001b[0;36mbuild_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m       \u001b[1;31m#feats = self.resnet(self.images)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m      \u001b[1;31m# feats = self.vggnet(self.images)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m       \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m       \u001b[1;31m#feats = self.DIY_network(self.images)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c9cfe0b8c492>\u001b[0m in \u001b[0;36msimple_network\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m    120\u001b[0m      \u001b[1;31m#conf st2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'st3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stride_arr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bn1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_leakiness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c9cfe0b8c492>\u001b[0m in \u001b[0;36m_batch_norm\u001b[1;34m(self, name, x)\u001b[0m\n\u001b[0;32m    307\u001b[0m       beta = tf.get_variable(\n\u001b[0;32m    308\u001b[0m           \u001b[1;34m'beta'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m           initializer=tf.constant_initializer(0.0, tf.float32))\n\u001b[0m\u001b[0;32m    310\u001b[0m       gamma = tf.get_variable(\n\u001b[0;32m    311\u001b[0m           \u001b[1;34m'gamma'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1465\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1215\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    525\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    479\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    846\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 848\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable embed/bn1/beta already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\KAIST\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\KAIST\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\KAIST\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## BUILD GRAPH\n",
    "model = Deepnet(hps, 'train')\n",
    "model.build_graph()\n",
    "\n",
    "## MAKE SESSION\n",
    "saver = tf.train.Saver()\n",
    "# config = tf.ConfigProto(allow_soft_placement=True)\n",
    "# config.gpu_options.allow_growth = True # Use memory as much as needed\n",
    "sess = tf.Session()\n",
    "\n",
    "## INITIALIZE SESSION\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "## HYPERPARAMETER\n",
    "lrn_rate = hps.lrn_rate\n",
    "max_step = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Updates parameters with back-propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_step+1):\n",
    "\n",
    "    ## STEP DECAYING\n",
    "    if step < 2000:\n",
    "        lrn_rate = hps.lrn_rate\n",
    "    elif step < 4000:\n",
    "        lrn_rate = 0.1 * hps.lrn_rate\n",
    "    else:\n",
    "        lrn_rate = 0.01 * hps.lrn_rate\n",
    "\n",
    "    ## BATCH SELECTION\n",
    "    k_start = hps.batch_size * step % train_images.shape[0]\n",
    "    k_end = hps.batch_size * step % train_images.shape[0] + hps.batch_size\n",
    "    batch_images = train_images[k_start:k_end, :, :, :]\n",
    "    batch_labels = train_labels[k_start:k_end, :]\n",
    "\n",
    "    ## RUN SESSION\n",
    "    start_time = time.time()\n",
    "    (_, loss, loss_cls, truth, prediction, train_step) = sess.run(\n",
    "        [model.train_op, model.cost, model.cost_cls, model.labels, model.prediction,\n",
    "        model.global_step],\n",
    "        feed_dict={model.lrn_rate: lrn_rate,\n",
    "                    model.images: batch_images,\n",
    "                    model.labels: batch_labels})\n",
    "    duration = time.time() - start_time\n",
    "    sec_per_batch = float(duration)\n",
    "\n",
    "    ## CALCULATE ACCURACY\n",
    "    truth = np.argmax(truth, axis=1)\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "    precision = np.mean(truth == prediction)\n",
    "    if step % 10 == 0:\n",
    "        print('  [*] TRAINING Iteration %d, Lr: %.3f, Loss: %.4f, Acc: %.4f (duration: %.3fs)'\n",
    "                             % (step, lrn_rate, loss_cls, precision, sec_per_batch))\n",
    "\n",
    "    ## VALIDATION\n",
    "    if step % 100 == 0:\n",
    "        total_prediction = 0\n",
    "        correct_prediction = 0\n",
    "        for k in range(100):\n",
    "            ## BATCH SELECTION\n",
    "            k_start = hps.batch_size * k % val_images.shape[0]\n",
    "            k_end = hps.batch_size * k % val_images.shape[0] + hps.batch_size\n",
    "            batch_images = val_images[k_start:k_end, :, :, :]\n",
    "            batch_labels = val_labels[k_start:k_end, :]\n",
    "            (loss, truth, prediction) = sess.run(\n",
    "                [model.cost_cls, model.labels, model.prediction],\n",
    "                feed_dict={model.images: batch_images,\n",
    "                          model.labels: batch_labels})\n",
    "\n",
    "            truth = np.argmax(truth, axis=1)\n",
    "            prediction = np.argmax(prediction, axis=1)\n",
    "            precision = np.sum(truth == prediction)\n",
    "            total_prediction += prediction.shape[0]\n",
    "            correct_prediction += precision\n",
    "\n",
    "        precision = 1.0 * correct_prediction / total_prediction\n",
    "        print('  [*] VALIDATION ACC: %.3f' % precision)\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        save_path = saver.save(sess, log_dir + \"iter_%d\" % (step))\n",
    "        print(' Model saved in file: %s.' % save_path)\n",
    "\n",
    "print('Optimization done.')\n",
    "print('Save the checkpoint.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load the test images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ MNIST INPUTS\n",
    "mnist = input_data.read_data_sets('./data/', one_hot=True)\n",
    "test_images = mnist.test.images\n",
    "test_labels = mnist.test.labels\n",
    "test_images = test_images.reshape([-1, 28, 28, 1])\n",
    "print(\"\\nThe number of test images: %d\" % (test_images.shape[0]))\n",
    "\n",
    "## Plot the 1st test image and label\n",
    "plt.imshow(test_images[0,:,:,0], cmap='Greys')\n",
    "print(\"\\nOne-hot labels for this image:\")\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Build a model and make a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "## BUILD GRAPH\n",
    "hps = HParams(batch_size=1, lrn_rate=base_lr, weight_decay_rate=0.0001)\n",
    "model = Deepnet(hps, 'test')\n",
    "model.build_graph()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "## MAKE SESSION \n",
    "## if there exist checkpoints, then restore it.\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "ckpt_state = tf.train.get_checkpoint_state(log_dir)\n",
    "if not (ckpt_state and ckpt_state.model_checkpoint_path):\n",
    "    print('No model to eval yet at %s' % log_dir)\n",
    "\n",
    "print('Loading checkpoint %s' % ckpt_state.model_checkpoint_path)\n",
    "saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
    "\n",
    "## HYPERPARAMETER\n",
    "batch_size = 1\n",
    "total_prediction = 0\n",
    "correct_prediction = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Check the prediction for the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, truth, prediction) = sess.run(\n",
    "    [model.cost, model.labels, model.prediction],\n",
    "    feed_dict={model.images: test_images[0,:,:,0].reshape(1,28,28,1),\n",
    "                model.labels: test_labels[0].reshape(1,10)})\n",
    "\n",
    "print(\"The prediction of the network is: %d\" % np.argmax(prediction, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Average the accuray for 500 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for step in range(500):\n",
    "\n",
    "    ## BATCH SELECTION\n",
    "    k_start = hps.batch_size * step % test_images.shape[0]\n",
    "    k_end = hps.batch_size * step % test_images.shape[0] + hps.batch_size\n",
    "    batch_images = test_images[k_start:k_end, :, :, :]\n",
    "    batch_labels = test_labels[k_start:k_end, :]\n",
    "\n",
    "    ### RUN SESSION\n",
    "    start_time = time.time()\n",
    "    (loss, truth, prediction) = sess.run(\n",
    "        [model.cost, model.labels, model.prediction],\n",
    "        feed_dict={model.images: batch_images,\n",
    "                    model.labels: batch_labels})\n",
    "\n",
    "    truth = np.argmax(truth, axis=1)\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "    precision = np.sum(truth == prediction)\n",
    "#     print('Loss: %.4f, Acc: %.4f' \n",
    "#                % (loss, precision))\n",
    "    total_prediction += prediction.shape[0]\n",
    "    correct_prediction += precision\n",
    "\n",
    "precision = 1.0 * correct_prediction / total_prediction\n",
    "print('Acc: %.3f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
