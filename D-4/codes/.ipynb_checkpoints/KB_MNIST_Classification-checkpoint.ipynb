{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Hand-written digit classification\n",
    "\n",
    "### **2018/10/13 KB-KAIST 인공지능 집중 강의**<br/>\n",
    "Authorized by SIIT, KAIST\n",
    "Yekang Lee, Jaemyung Yu, and Junmo Kim<br/>\n",
    "Converted to Jupyter notebook by Kiwon Lee\n",
    "\n",
    "***Tip> shotcuts for Jupyter Notebook***\n",
    "* Ctrl + Enter : run cell\n",
    "* Shift + Enter : run cell and select below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Libraries\n",
    "* Numpy: Fundamenta package for scientific computing with Python\n",
    "* Tensorflow: An open source machine learning library for research and production\n",
    "* Matplotlib: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.training import moving_averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "log_dir = 'logs/'\n",
    "batch_size = 100\n",
    "base_lr = 0.1\n",
    "\n",
    "HParams = namedtuple('HParams',\n",
    "                     'batch_size, lrn_rate, '\n",
    "                     'weight_decay_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a graph\n",
    "- Option 1: Resnet\n",
    "- Option 2: VGGnet\n",
    "- Option 3: Simple network (fill in the blank)\n",
    "- Option 4: DIY network (fill in the blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepnet(object):\n",
    "\n",
    "  def __init__(self, hps, mode):\n",
    "\n",
    "    self.hps = hps\n",
    "\n",
    "    ## HYPERPARAMETER\n",
    "    self.total_classes = 10\n",
    "    self.relu_leakiness = 0\n",
    "    self.mode = mode\n",
    "    self._extra_train_ops = []\n",
    "    self.num_residual_units = 3\n",
    "\n",
    "    ## PLACEHOLDERS\n",
    "    self.lrn_rate = tf.placeholder(tf.float32, shape=(), name='lrn_rate')\n",
    "    self.images = tf.placeholder(tf.float32, shape=(hps.batch_size, 28, 28, 1), name='images')\n",
    "    self.labels = tf.placeholder(tf.float32, shape=(hps.batch_size, self.total_classes), name='labels')\n",
    "\n",
    "\n",
    "  ## BUILD GRAPH\n",
    "  ## Option 1: resnet\n",
    "  ## Option 2: vggnet\n",
    "  ## Option 3: simple network (fill in the blank!)\n",
    "  ## Option 4: DIY network (fill in the blank!)\n",
    "\n",
    "  def build_graph(self):\n",
    "    ## GLOBAL ITERATION\n",
    "    self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "    ## FEATURE EXTRACTION\n",
    "    with tf.variable_scope('embed') as scope:\n",
    "      #feats = self.resnet(self.images)\n",
    "     # feats = self.vggnet(self.images)\n",
    "      feats = self.simple_network(self.images)\n",
    "      #feats = self.DIY_network(self.images)\n",
    "\n",
    "    ## LOGITS\n",
    "    logits = self._fully_connected('logits', feats, self.total_classes)\n",
    "\n",
    "    ## SOFTMAX \n",
    "    self.prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    ## COST FUNCTION (CROSS ENTROPY)\n",
    "    cent = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.labels)\n",
    "    self.cost_cls = tf.reduce_mean(cent, name='cent')\n",
    "\n",
    "    self.build_cost()\n",
    "\n",
    "    if self.mode == 'train':\n",
    "      self.build_train_op()\n",
    "\n",
    "\n",
    "  def build_cost(self):\n",
    "    if self.mode == 'train':\n",
    "      self.cost = self.cost_cls\n",
    "      self.cost += self.decay()\n",
    "    elif self.mode == 'test':\n",
    "      self.cost = self.cost_cls\n",
    "\n",
    "  ## OPTIMIZER (SGD+MOMENTUM)\n",
    "  def build_train_op(self):\n",
    "    optimizer = tf.train.MomentumOptimizer(self.lrn_rate, 0.9, use_nesterov=True)\n",
    "    train_vars = tf.trainable_variables()\n",
    "    grads = tf.gradients(self.cost, train_vars)\n",
    "    apply_op = optimizer.apply_gradients(zip(grads, train_vars),\n",
    "                                         global_step=self.global_step,\n",
    "                                         name='train_step')\n",
    "\n",
    "    train_ops = [apply_op] + self._extra_train_ops\n",
    "    self.train_op = tf.group(*train_ops)\n",
    "\n",
    "  ## L2 WEIGHT DECAY\n",
    "  def decay(self):\n",
    "    costs = []\n",
    "    for var in tf.trainable_variables():\n",
    "      if var.op.name.find(r'DW') > 0:\n",
    "        costs.append(tf.nn.l2_loss(var))\n",
    "\n",
    "    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))\n",
    "\n",
    "  ################################\n",
    "  ####### FILL IN THE BLANK ######\n",
    "  ################################\n",
    "  ################################\n",
    "  ####### BE CREATIVE! ###########\n",
    "  ################################\n",
    "  def DIY_network(self, images):\n",
    "    \"\"\" Your Code \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    img_feat = self._global_avg_pool(x)\n",
    "    return img_feat\n",
    "\n",
    "\n",
    "\n",
    "  ################################\n",
    "  ####### FILL IN THE BLANK ######\n",
    "  ################################\n",
    "  def simple_network(self, images):\n",
    "    \"\"\" Your Code \"\"\"\n",
    "\n",
    "    #conv st1\n",
    "    x1 = self._conv('st1', images, 3, 1, 16, self._stride_arr(1))\n",
    "    x1 = self._batch_norm('bn1', x1)\n",
    "    x1 = self._relu(x1, self.relu_leakiness)\n",
    "    \n",
    "     #conf st2\n",
    "    x2 = self._conv('st3', x1, 3, 16, 16, self._stride_arr(1))\n",
    "    x2 = self._batch_norm('bn1', x2)\n",
    "    x2 = self._relu(x2, self.relu_leakiness)\n",
    "\n",
    "    #conv st1\n",
    "    x1 = self._conv('st2', x1, 3, 16, 16, self._stride_arr(1))\n",
    "    x1 = self._batch_norm('bn1', x1)\n",
    "    x1 = self._relu(x1, self.relu_leakiness)    \n",
    "    \n",
    "    #avg pool st2\n",
    "    x1 = tf.nn.avg_pool(x1, self._stride_arr(2), self._stride_arr(2), 'VALID')    \n",
    "    \n",
    "   \n",
    "         \n",
    "    #Eltwise sum\n",
    "    x= x1 + x2\n",
    "    \n",
    "    #conv st1\n",
    "    x3 = self._conv('st4', x3, 3, 16, 16, self._stride_arr(1))\n",
    "    x3 = self._batch_norm('bn1', x3)\n",
    "    x3 = self._relu(x3, self.relu_leakiness)\n",
    "    \n",
    "    \n",
    "    #concatenate sum\n",
    "    x = tf.concat([x3, x], 3)    \n",
    "    \n",
    "    \n",
    "    #conv st1\n",
    "    x = self._conv('st5', X, 3, 32, 32, self._stride_arr(1))    \n",
    "            \n",
    "    x = self._batch_norm('bn1', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    \n",
    "    \n",
    "    \n",
    "    img_feat = self._global_avg_pool(x)\n",
    "    return img_feat\n",
    "\n",
    "  ## VGGNET\n",
    "  ## Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
    "  ## https://arxiv.org/abs/1409.1556\n",
    "\n",
    "  def vggnet(self, images, labels=None):\n",
    "    x = self._conv('conv1', images, 7, 1, 16, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn1', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    x = self._conv('conv2', x, 3, 16, 16, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn2', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    x = tf.nn.avg_pool(x, self._stride_arr(2), self._stride_arr(2), 'VALID')\n",
    "\n",
    "    x = self._conv('conv3', x, 3, 16, 32, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn3', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    x = self._conv('conv4', x, 3, 32, 32, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn4', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    x = tf.nn.avg_pool(x, self._stride_arr(2), self._stride_arr(2), 'VALID')\n",
    "\n",
    "    x = self._conv('conv5', x, 3, 32, 64, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn5', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    x = self._conv('conv6', x, 3, 64, 64, self._stride_arr(1))\n",
    "    x = self._batch_norm('bn6', x)\n",
    "    x = self._relu(x, self.relu_leakiness)\n",
    "    img_feat = self._global_avg_pool(x)\n",
    "    return img_feat\n",
    "\n",
    "\n",
    "  ## RESNET\n",
    "  ## Deep Residual Learning for Image Recognition\n",
    "  ## https://arxiv.org/abs/1512.03385\n",
    "    \n",
    "  def resnet(self, images, labels=None):\n",
    "\n",
    "    with tf.variable_scope('init'):\n",
    "      x = self._conv('conv1', images, 3, 1, 16, self._stride_arr(1))\n",
    "\n",
    "    strides = [1, 2, 2]\n",
    "    activate_before_residual = [True, False, False]\n",
    "    res_func = self.residual\n",
    "    filters = [16, 16, 32, 64]\n",
    "\n",
    "    with tf.variable_scope('unit_1_0'):\n",
    "      x = res_func(x, filters[0], filters[1], self._stride_arr(strides[0]),\n",
    "                   activate_before_residual[0])\n",
    "    for i in range(1, self.num_residual_units):\n",
    "      with tf.variable_scope('unit_1_%d' % i):\n",
    "        x = res_func(x, filters[1], filters[1], self._stride_arr(1), False)\n",
    "\n",
    "    with tf.variable_scope('unit_2_0'):\n",
    "      x = res_func(x, filters[1], filters[2], self._stride_arr(strides[1]),\n",
    "                   activate_before_residual[1])\n",
    "    for i in range(1, self.num_residual_units):\n",
    "      with tf.variable_scope('unit_2_%d' % i):\n",
    "        x = res_func(x, filters[2], filters[2], self._stride_arr(1), False)\n",
    "\n",
    "    with tf.variable_scope('unit_3_0'):\n",
    "      x = res_func(x, filters[2], filters[3], self._stride_arr(strides[2]),\n",
    "                   activate_before_residual[2])\n",
    "    for i in range(1, self.num_residual_units):\n",
    "      with tf.variable_scope('unit_3_%d' % i):\n",
    "        x = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\n",
    "    \n",
    "    with tf.variable_scope('unit_last'):\n",
    "      x = self._batch_norm('bn_last', x)\n",
    "      x = self._relu(x, self.relu_leakiness)\n",
    "      img_feat = self._global_avg_pool(x)\n",
    "      return img_feat\n",
    "\n",
    "\n",
    "  def residual(self, x, in_filter, out_filter, stride,\n",
    "                activate_before_residual=False):\n",
    "    \"\"\"Residual unit with 2 sub layers.\"\"\"\n",
    "    if activate_before_residual:\n",
    "      with tf.variable_scope('shared_activation'):\n",
    "        x = self._batch_norm('bn_init', x)\n",
    "        x = self._relu(x, self.relu_leakiness)\n",
    "        orig_x = x\n",
    "    else:\n",
    "      with tf.variable_scope('residual_only_activation'):\n",
    "        orig_x = x\n",
    "        x = self._batch_norm('bn_init', x)\n",
    "        x = self._relu(x, self.relu_leakiness)\n",
    "\n",
    "    with tf.variable_scope('sub1'):\n",
    "      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)\n",
    "\n",
    "    with tf.variable_scope('sub2'):\n",
    "      x = self._batch_norm('bn2', x)\n",
    "      x = self._relu(x, self.relu_leakiness)\n",
    "      x = self._conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n",
    "\n",
    "    with tf.variable_scope('sub_add'):\n",
    "      if in_filter != out_filter:\n",
    "        orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')\n",
    "        orig_x = tf.pad(\n",
    "            orig_x, [[0, 0], [0, 0], [0, 0],\n",
    "                     [(out_filter-in_filter)//2, (out_filter-in_filter)//2]])\n",
    "      x += orig_x\n",
    "\n",
    "    return x\n",
    "\n",
    "  ## STRIDE\n",
    "  def _stride_arr(self, stride):\n",
    "    return [1, stride, stride, 1]\n",
    "  \n",
    "    3,3,13\n",
    "  ## CONVOLUTIONAL LAYER\n",
    "  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n",
    "    with tf.variable_scope(name):\n",
    "      n = filter_size * filter_size * out_filters\n",
    "      w = tf.get_variable(\n",
    "          'weight/DW', [filter_size, filter_size, in_filters, out_filters],\n",
    "          tf.float32, initializer=tf.uniform_unit_scaling_initializer(factor=2.0))\n",
    "\n",
    "      y = tf.nn.conv2d(x, w, strides, padding='SAME')\n",
    "\n",
    "    return y\n",
    "\n",
    "  ## RELU LAYER\n",
    "  def _relu(self, x, leakiness=0.0):\n",
    "    return tf.maximum(x, leakiness*x)\n",
    "\n",
    "  ## FULLY-CONNECTED LAYER\n",
    "  def _fully_connected(self, name, x, out_dim, is_reuse=None):\n",
    "    with tf.variable_scope(name, reuse=is_reuse):\n",
    "      x = tf.reshape(x, [self.hps.batch_size, -1])\n",
    "      w = tf.get_variable(\n",
    "          'weight/DW', [x.get_shape()[1], out_dim],\n",
    "          initializer=tf.uniform_unit_scaling_initializer(factor=2.0))\n",
    "      b = tf.get_variable('bias/DW', [out_dim],\n",
    "                          initializer=tf.constant_initializer())\n",
    "      y = tf.nn.xw_plus_b(x, w, b)\n",
    "\n",
    "    return y\n",
    "\n",
    "  ## GLOBAL AVERAGE POOLING LAYER\n",
    "  def _global_avg_pool(self, x):\n",
    "    assert x.get_shape().ndims == 4\n",
    "    return tf.reduce_mean(x, [1, 2])\n",
    "\n",
    "  ## BATCH NORMALIZATION LAYER\n",
    "  def _batch_norm(self, name, x):\n",
    "    with tf.variable_scope(name):\n",
    "      params_shape = [x.get_shape()[-1]]\n",
    "\n",
    "      beta = tf.get_variable(\n",
    "          'beta', params_shape, tf.float32,\n",
    "          initializer=tf.constant_initializer(0.0, tf.float32))\n",
    "      gamma = tf.get_variable(\n",
    "          'gamma', params_shape, tf.float32,\n",
    "          initializer=tf.constant_initializer(1.0, tf.float32))\n",
    "\n",
    "      if self.mode == 'train':\n",
    "        mean, variance = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
    "\n",
    "        moving_mean = tf.get_variable(\n",
    "            'moving_mean', params_shape, tf.float32,\n",
    "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
    "            trainable=False)\n",
    "        moving_variance = tf.get_variable(\n",
    "            'moving_variance', params_shape, tf.float32,\n",
    "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
    "            trainable=False)\n",
    "\n",
    "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
    "            moving_mean, mean, 0.9))\n",
    "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
    "            moving_variance, variance, 0.9))\n",
    "      else:\n",
    "        mean = tf.get_variable(\n",
    "            'moving_mean', params_shape, tf.float32,\n",
    "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
    "            trainable=False)\n",
    "        variance = tf.get_variable(\n",
    "            'moving_variance', params_shape, tf.float32,\n",
    "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
    "            trainable=False)\n",
    "\n",
    "      # epsilon used to be 1e-5. Maybe 0.001 solves NaN problem in deeper net.\n",
    "      y = tf.nn.batch_normalization(\n",
    "          x, mean, variance, beta, gamma, 0.001)\n",
    "      y.set_shape(x.get_shape())\n",
    "      return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "hps = HParams(batch_size=batch_size, lrn_rate=base_lr, weight_decay_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 3.1 Load MNIST data and split train/val/test pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "One-hot labels for this image:\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## READ MNIST INPUTS\n",
    "mnist = input_data.read_data_sets('./data/', one_hot=True)\n",
    "\n",
    "## NUM_DATA\n",
    "NUM_DATA = 1000\n",
    "\n",
    "## TRAIN/VAL SPLITS\n",
    "train_images = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "train_images = train_images.reshape([-1, 28, 28, 1])\n",
    "train_images = train_images[0:NUM_DATA]\n",
    "train_labels = train_labels[0:NUM_DATA]\n",
    "\n",
    "val_images = mnist.validation.images\n",
    "val_labels = mnist.validation.labels\n",
    "val_images = val_images.reshape([-1, 28, 28, 1])\n",
    "\n",
    "## RANDOM SHUFFLING\n",
    "order = np.random.permutation(train_images.shape[0])\n",
    "order = order.astype(np.int32)\n",
    "train_images = train_images[order, :, :, :]\n",
    "train_labels = train_labels[order, :]\n",
    "\n",
    "## Plot the 1st image and label\n",
    "plt.imshow(train_images[0,:,:,0], cmap='Greys')\n",
    "print(\"\\nOne-hot labels for this image:\")\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'x' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2054e418ecc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## BUILD GRAPH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m## MAKE SESSION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e186ec2c1448>\u001b[0m in \u001b[0;36mbuild_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m       \u001b[1;31m#feats = self.resnet(self.images)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m      \u001b[1;31m# feats = self.vggnet(self.images)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m       \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m       \u001b[1;31m#feats = self.DIY_network(self.images)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e186ec2c1448>\u001b[0m in \u001b[0;36msimple_network\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;31m#conv st1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'st1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stride_arr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bn1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_leakiness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'x' referenced before assignment"
     ]
    }
   ],
   "source": [
    "## BUILD GRAPH\n",
    "model = Deepnet(hps, 'train')\n",
    "model.build_graph()\n",
    "\n",
    "## MAKE SESSION\n",
    "saver = tf.train.Saver()\n",
    "# config = tf.ConfigProto(allow_soft_placement=True)\n",
    "# config.gpu_options.allow_growth = True # Use memory as much as needed\n",
    "sess = tf.Session()\n",
    "\n",
    "## INITIALIZE SESSION\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "## HYPERPARAMETER\n",
    "lrn_rate = hps.lrn_rate\n",
    "max_step = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Updates parameters with back-propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_step+1):\n",
    "\n",
    "    ## STEP DECAYING\n",
    "    if step < 2000:\n",
    "        lrn_rate = hps.lrn_rate\n",
    "    elif step < 4000:\n",
    "        lrn_rate = 0.1 * hps.lrn_rate\n",
    "    else:\n",
    "        lrn_rate = 0.01 * hps.lrn_rate\n",
    "\n",
    "    ## BATCH SELECTION\n",
    "    k_start = hps.batch_size * step % train_images.shape[0]\n",
    "    k_end = hps.batch_size * step % train_images.shape[0] + hps.batch_size\n",
    "    batch_images = train_images[k_start:k_end, :, :, :]\n",
    "    batch_labels = train_labels[k_start:k_end, :]\n",
    "\n",
    "    ## RUN SESSION\n",
    "    start_time = time.time()\n",
    "    (_, loss, loss_cls, truth, prediction, train_step) = sess.run(\n",
    "        [model.train_op, model.cost, model.cost_cls, model.labels, model.prediction,\n",
    "        model.global_step],\n",
    "        feed_dict={model.lrn_rate: lrn_rate,\n",
    "                    model.images: batch_images,\n",
    "                    model.labels: batch_labels})\n",
    "    duration = time.time() - start_time\n",
    "    sec_per_batch = float(duration)\n",
    "\n",
    "    ## CALCULATE ACCURACY\n",
    "    truth = np.argmax(truth, axis=1)\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "    precision = np.mean(truth == prediction)\n",
    "    if step % 10 == 0:\n",
    "        print('  [*] TRAINING Iteration %d, Lr: %.3f, Loss: %.4f, Acc: %.4f (duration: %.3fs)'\n",
    "                             % (step, lrn_rate, loss_cls, precision, sec_per_batch))\n",
    "\n",
    "    ## VALIDATION\n",
    "    if step % 100 == 0:\n",
    "        total_prediction = 0\n",
    "        correct_prediction = 0\n",
    "        for k in range(100):\n",
    "            ## BATCH SELECTION\n",
    "            k_start = hps.batch_size * k % val_images.shape[0]\n",
    "            k_end = hps.batch_size * k % val_images.shape[0] + hps.batch_size\n",
    "            batch_images = val_images[k_start:k_end, :, :, :]\n",
    "            batch_labels = val_labels[k_start:k_end, :]\n",
    "            (loss, truth, prediction) = sess.run(\n",
    "                [model.cost_cls, model.labels, model.prediction],\n",
    "                feed_dict={model.images: batch_images,\n",
    "                          model.labels: batch_labels})\n",
    "\n",
    "            truth = np.argmax(truth, axis=1)\n",
    "            prediction = np.argmax(prediction, axis=1)\n",
    "            precision = np.sum(truth == prediction)\n",
    "            total_prediction += prediction.shape[0]\n",
    "            correct_prediction += precision\n",
    "\n",
    "        precision = 1.0 * correct_prediction / total_prediction\n",
    "        print('  [*] VALIDATION ACC: %.3f' % precision)\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        save_path = saver.save(sess, log_dir + \"iter_%d\" % (step))\n",
    "        print(' Model saved in file: %s.' % save_path)\n",
    "\n",
    "print('Optimization done.')\n",
    "print('Save the checkpoint.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load the test images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ MNIST INPUTS\n",
    "mnist = input_data.read_data_sets('./data/', one_hot=True)\n",
    "test_images = mnist.test.images\n",
    "test_labels = mnist.test.labels\n",
    "test_images = test_images.reshape([-1, 28, 28, 1])\n",
    "print(\"\\nThe number of test images: %d\" % (test_images.shape[0]))\n",
    "\n",
    "## Plot the 1st test image and label\n",
    "plt.imshow(test_images[0,:,:,0], cmap='Greys')\n",
    "print(\"\\nOne-hot labels for this image:\")\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Build a model and make a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "## BUILD GRAPH\n",
    "hps = HParams(batch_size=1, lrn_rate=base_lr, weight_decay_rate=0.0001)\n",
    "model = Deepnet(hps, 'test')\n",
    "model.build_graph()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "## MAKE SESSION \n",
    "## if there exist checkpoints, then restore it.\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "ckpt_state = tf.train.get_checkpoint_state(log_dir)\n",
    "if not (ckpt_state and ckpt_state.model_checkpoint_path):\n",
    "    print('No model to eval yet at %s' % log_dir)\n",
    "\n",
    "print('Loading checkpoint %s' % ckpt_state.model_checkpoint_path)\n",
    "saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
    "\n",
    "## HYPERPARAMETER\n",
    "batch_size = 1\n",
    "total_prediction = 0\n",
    "correct_prediction = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Check the prediction for the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, truth, prediction) = sess.run(\n",
    "    [model.cost, model.labels, model.prediction],\n",
    "    feed_dict={model.images: test_images[0,:,:,0].reshape(1,28,28,1),\n",
    "                model.labels: test_labels[0].reshape(1,10)})\n",
    "\n",
    "print(\"The prediction of the network is: %d\" % np.argmax(prediction, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Average the accuray for 500 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for step in range(500):\n",
    "\n",
    "    ## BATCH SELECTION\n",
    "    k_start = hps.batch_size * step % test_images.shape[0]\n",
    "    k_end = hps.batch_size * step % test_images.shape[0] + hps.batch_size\n",
    "    batch_images = test_images[k_start:k_end, :, :, :]\n",
    "    batch_labels = test_labels[k_start:k_end, :]\n",
    "\n",
    "    ### RUN SESSION\n",
    "    start_time = time.time()\n",
    "    (loss, truth, prediction) = sess.run(\n",
    "        [model.cost, model.labels, model.prediction],\n",
    "        feed_dict={model.images: batch_images,\n",
    "                    model.labels: batch_labels})\n",
    "\n",
    "    truth = np.argmax(truth, axis=1)\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "    precision = np.sum(truth == prediction)\n",
    "#     print('Loss: %.4f, Acc: %.4f' \n",
    "#                % (loss, precision))\n",
    "    total_prediction += prediction.shape[0]\n",
    "    correct_prediction += precision\n",
    "\n",
    "precision = 1.0 * correct_prediction / total_prediction\n",
    "print('Acc: %.3f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
